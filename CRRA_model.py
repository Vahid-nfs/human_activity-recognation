import pandas as pd import numpy as npimport matplotlib.pyplot as plt from  sklearn.model_selection import train_test_split ,KFoldimport kerasfrom keras import Model,layers ,Sequentialfrom tqdm import tqdmfrom sklearn.metrics import confusion_matriximport seaborn as snsimport osfrom sklearn.metrics import auc , roc_curvefrom keras_self_attention import SeqSelfAttentionfrom qhoptim.tf import QHMOptimizer, QHAdamOptimizerimport tensorflow as tfnum_epochs=60w_df=pd.read_pickle("./dataset/watch.pkl").sort_values(["Subject_id","Activity","Timestamp"],ignore_index=True)p_df=pd.read_pickle("./dataset/phone.pkl").sort_values(["Subject_id","Activity","Timestamp"],ignore_index=True)def activity_mapper(activity):    if(activity in ['A', 'B', 'C', 'D', 'E']):        return 0    else:(activity in ['M','P', 'O', 'F', 'Q', 'R','G', 'S'])    return 1def make_window(df):    window_size = 200    stride = 50    frames = []    labels=[]    for i in tqdm(range(0, len(df)-window_size, stride)):        window = df.iloc[i:i+window_size]        if window['Activity'].nunique():          frames.append(np.expand_dims(window[["XA","YA","ZA","XG","YG","ZG"]].values,0))          labels.append(np.unique(window['Activity_type'])[0])    return np.vstack(frames).astype(dtype=np.float64),np.hstack(labels)class Res_Atten_Model(Model):    def __init__(self):        super().__init__()        self.Name="CRRA"        self.conv1=layers.Conv1D(16,7,activation="gelu",padding="same")                self.conv2=layers.Conv1D(32,5,activation="relu",padding="same")        self.conv3=layers.Conv1D(32,5,activation="relu",padding="same")                self.conv4=layers.Conv1D(64,3,activation="relu",padding="same")        self.conv5=layers.Conv1D(64,3,activation="relu",padding="same")        self.conv6=layers.Conv1D(64,3,activation="relu",padding="same")                self.conv7=layers.Conv1D(128,3,activation="relu",padding="same")        self.conv8=layers.Conv1D(128,3,activation="relu",padding="same")        self.conv9=layers.Conv1D(128,3,activation="relu",padding="same")                self.bd_lstm1=layers.Bidirectional(layers.LSTM(128,return_sequences=True))        self.bd_lstm2=layers.Bidirectional(layers.LSTM(128,return_sequences=True))        self.attention=SeqSelfAttention(attention_activation='sigmoid')                self.conv10=layers.Conv1D(128,3,activation="relu",padding="same")        self.conv11=layers.Conv1D(128,3,activation="relu",padding="same")                self.fc1=layers.Dense(64,activation="relu")        self.bn=layers.BatchNormalization()        self.fc2=layers.Dense(1,activation="sigmoid")        self.dropout=layers.Dropout(.1)                self.fltten=layers.Flatten()        self.Mxpool1=layers.MaxPooling1D(2)        self.Avgpool1=layers.AveragePooling1D(2)                    def block(self,x):        x=self.conv1(x)        x=self.Mxpool1(x)        x=self.conv2(x)        dx=self.conv3(x)        x=layers.add((x,dx))        x=self.Mxpool1(x)        x=self.conv4(x)        dx=self.conv5(x)        dx=self.conv6(dx)        x=layers.add((x,dx))        x=self.Mxpool1(x)                x=self.conv7(x)        dx=self.conv8(x)        dx=self.conv9(dx)        x=layers.add((x,dx))        x=self.Mxpool1(x)        return(x)            def call(self,x):                x1=self.block(x)        x2=self.block(x)        x3=self.block(x)                x=layers.concatenate((x1,x2,x3))        x=self.bd_lstm1(x)        x=self.bd_lstm2(x)        x=self.attention(x)                x=self.conv10(x)        x=self.conv11(x)        x=self.Avgpool1(x)        x=self.fltten(x)        x=self.dropout(self.bn(self.fc1(x)))        x=self.fc2(x)        return(x)def plot(history,title1=None,title2=None,f_name=None):    loss=history.history["loss"]    val_loss=history.history["val_loss"]    accuracy=history.history["accuracy"]    val_accuracy=history.history["val_accuracy"]    epochs=range(1,len(loss)+1)    fig,(ax1,ax2)=plt.subplots(nrows=1,ncols=2,figsize=(16,5))        ax1.plot(epochs , loss , "--" ,c="cyan" , label="training loss")    ax1.plot(epochs , val_loss , c="orange" , label="validation loss")    ax1.set_xlabel="epoch"    ax1.set_ylabel="loss"    ax1.set_title(title1)    ax1.legend()    ax2.plot(epochs , accuracy , "--" ,c="green" ,label="training accuracy")    ax2.plot(epochs , val_accuracy , c="red" , label="validation accuracy")    ax2.set_xlabel="epoch"    ax2.set_ylabel="accuracy"    ax2.set_title(title2)    ax2.legend()    fig.show()    if f_name:        fig.savefig(f_name,dpi=600)def plot_confusion_matrix(true,pred,title=None,f_name=None):    fig,ax=plt.subplots(figsize = (9,7))    sns.heatmap(confusion_matrix(true,pred),                yticklabels=["General Non Hand Oriented","General Hand Oriented"],                xticklabels=["General Non Hand Oriented","General Hand Oriented"],                 annot=True, fmt='d')    ax.set_xlabel('Predicted')    ax.set_ylabel('Truth')    ax.set_title(title)    if f_name:        plt.savefig(f_name,dpi=600)    plt.show()def ROC_curve_plot(fpr,tpr ,title=None ,f_name=None):    fig,ax=plt.subplots(figsize = (9,7))    roc_auc = auc(fpr, tpr)    ax.set_title('Receiver Operating Characteristic')    ax.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)    ax.legend(loc = 'lower right')    ax.plot([0, 1], [0, 1],'r--')    ax.set_xlim([0, 1])    ax.set_ylim([0, 1])    ax.set_ylabel('True Positive Rate')    ax.set_xlabel('False Positive Rate')    ax.set_title(title)    if f_name:        fig.savefig(f_name,dpi=600)        df_dict={"Smartphone":p_df,"Smartwatch":w_df}for df_name in df_dict.keys():    df=df_dict[df_name]    df["Activity_type"]=df['Activity'].apply(activity_mapper)    X,y=make_window(df)    # X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.33,random_state=10)        kf = KFold(n_splits=3)    for fold,(train_index, test_index) in enumerate(kf.split(X)):          X_train,y_train=X[train_index],y[train_index]        X_test,y_test=X[test_index],y[test_index]                model=Res_Atten_Model()        save_path=f"./result/{model.Name}/{df_name}/fold_{fold+1}"        os.makedirs(save_path,exist_ok=True)                optimizer = QHAdamOptimizer(learning_rate=1e-3, nu1=0.7, nu2=1.0, beta1=0.995, beta2=0.999)        model.compile(loss='binary_crossentropy',                         optimizer=optimizer,                        metrics=['accuracy'])                history=model.fit(X_train,y_train ,epochs=num_epochs,batch_size=128 ,                          validation_data=(X_test,y_test))                plot(history,title1=f"loss on {df_name} of {model.Name}",title2=f"Accuracy on {df_name} of {model.Name}",f_name=f"{save_path}/loss_acc.jpg")                y_score=np.squeeze(model.predict(X_test))        pred=np.vectorize(int)(y_score.round())                plot_confusion_matrix(y_test,pred,title=f"Confusion matrix for {df_name} of {model.Name}",f_name=f"{save_path}/Confusion.jpg")                fpr, tpr, thresholds = roc_curve(y_test, y_score)        ROC_curve_plot(fpr, tpr ,title=f"ROC Curve for {df_name} of {model.Name}" ,f_name=f"{save_path}/ROC_curv.jpg")        res_df=pd.DataFrame(history.history,columns=["loss","accuracy","val_loss","val_accuracy"])        res_df["epoch"]=range(1,len(res_df)+1)        res_df=res_df[["epoch","loss","accuracy","val_loss","val_accuracy"]]        res_df.to_csv(f"{save_path}/res_df.csv",index=False)                pred_df=pd.DataFrame(zip(y_test,pred,y_score),columns=["true_value","pred_vale","y_score"])             pred_df.to_csv(f"{save_path}/pred_df.csv",index=False)                                # from tensorflow.keras.utils import plot_model# plot_model(model, to_file='model.png', show_shapes = True, show_layer_names = True)    